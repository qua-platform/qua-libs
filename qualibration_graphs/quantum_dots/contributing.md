# Contributing to Quantum Dots QUAlibration Graphs

This directory contains QUAlibration nodes and utilities for quantum dots. We welcome fixes, new nodes, and improvements.

## Quick Start

1. Create or reuse a uv environment:
   ```bash
   uv venv .venv --python 3.xx
   ```
2. Install repo dev tooling:
   ```bash
   uv sync --dev
   ```
3. Install runtime dependencies:
   ```bash
   uv pip install -r requirements.txt
   ```
4. Install QUAM/QUAlibrate dependencies used by quantum_dots:
   ```bash
   uv pip install qualibrate
   uv pip install "qualang-tools @ git+https://github.com/qua-platform/py-qua-tools.git@feat/quantum-dots"
   uv pip install "git+https://github.com/qua-platform/qualibration-libs.git"
   ```
5. Install `quam-builder` from the quantum_dots branch:
   ```bash
   uv pip install -e /path/to/quam-builder@feat/quantum_dots
   ```

## Running Tests

Run a single simulation test:
```bash
uv run pytest tests/simulation/qualibration_graphs/quantum_dots/calibrations/loss_divincenzo/test_09b_time_rabi_chevron_parity_diff_sim.py -vv -s -rs
```

Prerequisites:
- QM SaaS credentials at `tests/.qm_saas_credentials.json` with `email`, `password`, optional `host`.
- Network access to QM SaaS.
- A working `quam-builder` checkout on `feat/quantum_dots`.

Optional skip control:
- By default, simulation tests run if prerequisites exist.
- To explicitly skip simulation tests: `RUN_SIM_TESTS=0 uv run pytest -m simulation`

## Testing Infrastructure Overview

Key pieces in `tests/simulation/`:
- `conftest.py` provides:
  - Programmatic QUAM construction for quantum dots
  - Node loader with patching to avoid auto-running actions on import
  - QM SaaS credentials loader
  - Small-sweep parameter overrides for fast simulation
  - Artifact generation helpers (plot + README)
- Simulation tests live under:
  - `tests/simulation/qualibration_graphs/quantum_dots/calibrations/...`

### Artifact Generation

Simulation tests generate artifacts in:
```
tests/simulation/artifacts/<node_name>/
```
By default, each test produces:
- `simulation.png` (plot of simulated samples)
- `README.md` (node summary + parameters)

These are generated by the `save_simulation_plot` and `markdown_generator` fixtures.

## Template: New Node Simulation Test

Copy this pattern when adding a new node test:
```python
from __future__ import annotations

import os
from pathlib import Path

import pytest

TEST_ROOT = Path(__file__).resolve().parent
REPO_ROOT = None
for parent in [TEST_ROOT, *TEST_ROOT.parents]:
    if (parent / "qualibration_graphs").is_dir() and (parent / "tests").is_dir():
        REPO_ROOT = parent
        break
if REPO_ROOT is None:
    REPO_ROOT = TEST_ROOT.parents[0]

NODE_PATH = (
    REPO_ROOT
    / "qualibration_graphs"
    / "quantum_dots"
    / "calibrations"
    / "<your_node_folder>"
    / "<your_node_file>.py"
)
ARTIFACTS_SUBDIR = "<your_node_name>"


@pytest.mark.simulation
def test_<your_node_name>_simulation(
    simulation_test_context,
    save_simulation_plot,
    markdown_generator,
):
    run_sim_tests = os.environ.get("RUN_SIM_TESTS")
    if run_sim_tests is not None and run_sim_tests != "1":
        pytest.skip("Simulation tests disabled. Set RUN_SIM_TESTS=1 to run.")

    ctx = simulation_test_context(NODE_PATH, ARTIFACTS_SUBDIR)
    ctx.configure_small_sweep()

    ctx.loaded_node.get_action("custom_param")()
    ctx.loaded_node.get_action("create_qua_program")()

    config = ctx.machine.generate_config()
    qua_program = ctx.loaded_node.node.namespace["qua_program"]

    from qm import QuantumMachinesManager, SimulationConfig
    import qm_saas

    client = qm_saas.QmSaas(
        email=ctx.credentials["email"],
        password=ctx.credentials["password"],
        host=ctx.credentials["host"],
    )
    client.close_all()

    with client.simulator(client.latest_version()) as instance:
        qmm = QuantumMachinesManager(
            host=instance.host,
            port=instance.port,
            connection_headers=instance.default_connection_headers,
            timeout=120,
        )
        simulation_config = SimulationConfig(duration=10_000)
        job = qmm.simulate(config, qua_program, simulation_config)
        job.wait_until("Done", timeout=60)
        simulated_samples = job.get_simulated_samples()

    save_simulation_plot(simulated_samples, ctx.artifacts_dir, title="<your title>")
    markdown_generator(ctx.loaded_node, ctx.get_parameters_dict(), ctx.artifacts_dir)

    assert (ctx.artifacts_dir / "simulation.png").exists()
    assert (ctx.artifacts_dir / "README.md").exists()
```

## Pre-commit Hooks

Install and run pre-commit checks:
```bash
uv run pre-commit install
uv run pre-commit run --all-files
```

## GitHub Actions

CI workflows live in `.github/workflows/`.
Simulation tests are opt-in (marked `@pytest.mark.simulation`) and are excluded from GitHub Actions by default.
If you want to run them locally, use `RUN_SIM_TESTS=1` and the `-m simulation` marker.

Only unit and interop tests are run in GitHub Actions in the current setup.

## Branch Names and Commit Messages

Branch names must start with:
`feature/`, `bugfix/`, `hotfix/`, `chore/`, `refactor/`, `experiment/`, or `release/`.

Commit messages must follow Commitizen (Conventional Commits) style, for example:
```
feat(quantum_dots): add new chevron node
fix(simulation): retry sample pull on transient errors
```

## Test Coverage Tiers

This is the planned coverage model (current CI only runs unit + interop):

- **Unit**: pure Python logic, helpers, parsing
  - Runs: every PR
  - Marker/path: `tests/unit/`, `@pytest.mark.unit`
  - Gating: required for merge
- **Analysis integration**: synthetic/recorded data pipelines
  - Runs: every PR (subset), nightly (full)
  - Marker/path: `tests/analysis/`, `@pytest.mark.analysis`
  - Gating: required for merge (subset)
- **Controller API / contracts**: mocked/dry-run backend
  - Runs: every PR
  - Marker/path: `tests/interop/contract_*.py`, `@pytest.mark.interop`
  - Gating: required for merge
- **Signal validation (sim)**: compile/simulate sequences
  - Runs: every PR (smoke), nightly (full)
  - Marker/path: `tests/simulation/`, `@pytest.mark.simulation`
  - Gating: required for merge (smoke)
- **Hardware sanity**
  - Runs: nightly/manual
  - Marker/path: `tests/hw/`, `@pytest.mark.hw`
  - Gating: required for release
- **Perf / scaling**
  - Runs: nightly/manual
  - Marker/path: `tests/perf/`, `@pytest.mark.perf`
  - Gating: must not regress beyond threshold

## Contribution Workflow

1. Create a branch for your changes.
2. Make your edits and run relevant tests.
3. Open a PR with a short summary and test results.

If you need help, open an issue or ask in your PR.
